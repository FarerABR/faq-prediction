# faq-prediction — مقایسه STS و LLM

این گزارش در راستای پروژه پیش‌بینی/رده‌بندی سوالات متداول بانکی (FAQ Matching) با دو رویکرد **STS** و **LLM** تهیه شده است.

هدف پروژه: برای هر سوال کاربر (در شیت `samples`) یک `idx` از شیت `faq` پیش‌بینی شود و سپس با برچسب واقعی `gt_idx` مقایسه گردد.
در این پروژه برای تصحیح و تکمیل کد به صورت محدود از GPT-5.1-codex استفاده شده است.

## 1) معیارهای ارزیابی (Metrics)

در این پروژه معیار اصلی **Top‑1** است.

### Top‑1 Accuracy و Top‑1 Error

اگر برای نمونه‌ی $i$ برچسب واقعی $y_i$ و پیش‌بینی $\hat{y}_i$ باشد:

$$
	{Accuracy} = \frac{1}{N}\sum_{i=1}^{N} \mathbf{1}[\hat{y}_i = y_i]
$$

$$
	{Error} = \frac{1}{N}\sum_{i=1}^{N} \mathbf{1}[\hat{y}_i \ne y_i] = 1 - \text{Accuracy}
$$

### Top‑K Accuracy (Hit@K / Acc@K)

اگر $R_i^{(K)}$ مجموعه/لیست $K$ پیش‌بینی برتر برای نمونه‌ی $i$ باشد:

$$
	{Acc@K} = \frac{1}{N}\sum_{i=1}^{N}\mathbf{1}[y_i \in R_i^{(K)}]
$$
---

## 2) روش اول — STS (Embedding Similarity)

### ایده کلی

در STS، متن‌ها به بردارهای عددی (Embedding) تبدیل می‌شوند و سپس با معیار شباهت (که در اینجا فاصله کسینوسی است) نزدیک‌ترین سوال FAQ انتخاب می‌شود.

### مدل استفاده‌شده

- مدل امبدینگ چندزبانه مناسب فارسی: `intfloat/multilingual-e5-base`

در E5 باید نقش‌ها مشخص باشد:

- سوال‌های FAQ به صورت `passage: ...`
- سوال کاربر به صورت `query: ...`

### پیش‌پردازش فارسی

برای کاهش نویز و افزایش شباهت معنایی، نرمال‌سازی انجام شد:

- تبدیل `ي`→`ی` و `ك`→`ک`
- حذف/تبدیل برخی کاراکترهای نامرئی مثل \u200c و…
- یکدست‌سازی فاصله‌ها

### محاسبه شباهت و پیش‌بینی

اگر بردار FAQ شماره $j$ را $e(f_j)$ و بردار نمونه $i$ را $e(q_i)$ بگیریم:

$$
s_{j,i} = \cos(e(f_j), e(q_i))
$$
 چون `normalize_embeddings=True` است، بردارها واحد می‌شوند و کسینوس شباهت برابر ضرب داخلی است.

پیش‌بینی Top‑1:

$$
\hat{y}_i = \text{idx}\left(\arg\max_j s_{j,i}\right)
$$

و امتیاز Top‑1:

$$
s_i = \max_j s_{j,i}
$$

### خروجی‌ها

در شیت `samples` ستون‌های زیر اضافه شدند:

- `sts_idx`: پیش‌بینی `idx`
- `sts_score`: امتیاز شباهت Top‑1

---

## 3) روش دوم — LLM (مدل زبانی بزرگ) با Prompting

### ایده کلی

در رویکرد LLM، برای هر نمونه:

- کل لیست FAQ (با `idx` و متن سوال) + سوال کاربر در Prompt قرار می‌گیرد.
- مدل باید دقیقاً یک `idx` را انتخاب کند.

از آنجا که تعداد FAQها کم است (۲۴ ردیف)، محدودیت طول کانتکست مشکل‌ساز نیست.

برای تست این روش از دو مدل و پرامپت هایی با دو زبان فارسی و انگلیسی استفاده شد.

### زیرساخت اجرا

برای اجرای مدل ها از سیستمی با 48 گیگابایت رم و 8 هسته cpu و کارت گرافیک L4 استفاده شد

### مدل‌ها

برای مقایسه، حداقل دو مدل پیشنهاد شد:

- `google/gemma-3-4b-it`
- `Qwen/Qwen2.5-7B-Instruct`

### Prompt و قالب خروجی

برای کاهش خطای parse و افزایش پایایی:

- خروجی باید فقط JSON باشد:

```json
{ "idx": 12 }
```

و پارامترهای تولید به صورت تعیین‌پذیر تنظیم می‌شوند (مثل `temperature=0`).

### چرایی `parse_ok`

ممکن است LLM خروجی غیرقابل‌ پارس بدهد (متن اضافه، JSON ناقص، چند مقدار idx و…)
بنابراین در کنار دقت، نرخ قابل‌پارس بودن نیز گزارش می‌شود:

$$
	{parse\_ok\_rate} = \frac{1}{N}\sum_{i=1}^{N}\mathbf{1}[\text{parse\_ok}_i]
$$

### چالش ها
دو مدل نامبرده شده با یک پرامپت ساده به خروجی مناسبی نمیرسیدند که عمدتاً مشکل از parse خروجی بود. برای حل این مشکل از چند پرامپت مختلف استفاده شد. در نهایت بهترین روش استفاده از چند نمونه یا تکنیک few-shot بود.

با این روش هر دو مدل در زبان انگلیسی به parse_rate=1 رسیدند. همچنین برای اطمینان یک بخش retry هم استفاده شد که اگر برای یکی از نمونه های خروجی مناسب نبود دوباره توسط مدل تست شود.
با توجه به نتایج پایین این نشان میدهد که پرامپت های انگلیسی با مثال بهترین گزینه بودند.

---

## 4) نتایج

### نتایج STS

- Top‑1 Accuracy: **0.5914**
- Top‑1 Error: **0.4086**
- Acc@3: **0.8387**


### نتایج LLM (برای هر مدل جدا)


EN - gemma-3-4b-it:

- Overall Accuracy (خروجی‌های نامعتبر هم به‌عنوان اشتباه): **0.7849**
- parse_ok_rate: **1.0**
- Error: **0.2150**
- Run time(ms/sample): **13.89**

EN - Qwen2.5-7B-Instruct:

- Overall Accuracy (خروجی‌های نامعتبر هم به‌عنوان اشتباه): **0.8064**
- parse_ok_rate: **1.0**
- Error: **0.1935**
- Run time(ms/sample): **40.63**

FA - gemma-3-4b-it:

- Overall Accuracy (خروجی‌های نامعتبر هم به‌عنوان اشتباه): **0.7311**
- parse_ok_rate: **1.0**
- Error: **0.2688**
- Run time(ms/sample): **13.68**

FA - Qwen2.5-7B-Instruct:

- Overall Accuracy (خروجی‌های نامعتبر هم به‌عنوان اشتباه): **0.6774**
- parse_ok_rate: **0.8924**
- Error: **0.3225**
- Run time(ms/sample): **44.44**

---

## 5) مقایسه مدل ها و جمع بندی

با توجه به نتایج به دست آمده مدل های زبانی نتایج بهتری کسب کردند و حتی در مواردی خطای top-1 آنها مشابه خطای top-3 مدل STS بود.
در مورد زبان و مدل های زبانی، مدل Qwen در زبان انگلیسی خروجی بهتری کسب کرده است. در صورتی که مدل gemma بر روی هر دو زبان خروجی مناسبی دارد و سرعت پردازش هر پرامپ بسیار پایین تر از مدل Qwen است(تقریبا 2.9 برابر سریع تر!)


---